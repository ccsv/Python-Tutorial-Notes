{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1 align=\"center\">NLP Tutorial in Gensim</h1>\n",
    "<h3 align=\"center\">Chuck Chan</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We are running a tutorial on Natural Language Processing in Python.', 'San Francisco will be foggy all week!', 'Python is a high level programming language.', 'Today it is cold in SF.', 'It is so sunny that you can fry an egg outside in Vegas.', 'Las Vegas is getting very cold in the evening.', 'Other programming languages that have Natural Language Processing libraries include Java and C.', 'The cold weather yesterday evening gave Kevin frostbite.', 'What is the weather outside? Is it rainy or sunny?', \"Doesn't it cost $25 to run your code on Amazon? No\", 'Running NLP code is sometimes very slow and not very accurate.', 'Python libraries for NLP include: Spacy, NLTK, and gensim', 'Gensim is is used in this tutorial because it is easy to install']\n"
     ]
    }
   ],
   "source": [
    "docs =[\"We are running a tutorial on Natural Language Processing in Python.\",\n",
    "       \"San Francisco will be foggy all week!\", \n",
    "       \"Python is a high level programming language.\",\n",
    "       \"Today it is cold in SF.\",\n",
    "       \"It is so sunny that you can fry an egg outside in Vegas.\",\n",
    "       \"Las Vegas is getting very cold in the evening.\",\n",
    "       \"Other programming languages that have Natural Language Processing libraries include Java and C.\",\n",
    "       \"The cold weather yesterday evening gave Kevin frostbite.\",\n",
    "       \"What is the weather outside? Is it rainy or sunny?\",\n",
    "       \"Doesn't it cost $25 to run your code on Amazon? No\",\n",
    "       \"Running NLP code is sometimes very slow and not very accurate.\",\n",
    "       \"Python libraries for NLP include: Spacy, NLTK, and gensim\",\n",
    "       \"Gensim is is used in this tutorial because it is easy to install\",\n",
    "    ]\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tokenize</h1>\n",
    "<p>Fill in the hash marks with regex pattern to extract punctuation</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are running a tutorial on Natural Language Processing in Python\n",
      "San Francisco will be foggy all week\n",
      "Python is a high level programming language\n",
      "Today it is cold in SF\n",
      "It is so sunny that you can fry an egg outside in Vegas\n",
      "Las Vegas is getting very cold in the evening\n",
      "Other programming languages that have Natural Language Processing libraries include Java and C\n",
      "The cold weather yesterday evening gave Kevin frostbite\n",
      "What is the weather outside Is it rainy or sunny\n",
      "Doesn't it cost $25 to run your code on Amazon No\n",
      "Running NLP code is sometimes very slow and not very accurate\n",
      "Python libraries for NLP include Spacy NLTK and gensim\n",
      "Gensim is is used in this tutorial because it is easy to install\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replace punctuation\n",
    "import re\n",
    "for i in range(len(docs)):\n",
    "    docs[i]= re.sub('(\\w+)(############)','\\g<1>', docs[i])\n",
    "    print(docs[i])\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dealing with compound Words</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are running a tutorial on nlp in Python\n",
      "san_francisco will be foggy all week\n",
      "Python is a high level programming language\n",
      "Today it is cold in san_francisco\n",
      "It is so sunny that you can fry an egg outside in las_vegas\n",
      "las_vegas is getting very cold in the evening\n",
      "Other programming languages that have nlp libraries include Java and C\n",
      "The cold weather yesterday evening gave Kevin frostbite\n",
      "What is the weather outside Is it rainy or sunny\n",
      "Doesn't it cost $25 to run your code on Amazon No\n",
      "Running nlp code is sometimes very slow and not very accurate\n",
      "Python libraries for nlp include Spacy NLTK and gensim\n",
      "Gensim is is used in this tutorial because it is easy to install\n"
     ]
    }
   ],
   "source": [
    "replacements ={\n",
    "    'san_francisco':['San Francisco', 'SF'],\n",
    "    'las_vegas':['Las Vegas',\"Vegas\"],\n",
    "    'nlp':['NLP', 'Natural Language Processing']\n",
    "}\n",
    "\n",
    "    \n",
    "    \n",
    "for key, value in replacements.items():\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(value)):\n",
    "            docs[i] = docs[i].replace(value[j], key)\n",
    "            \n",
    "for d in docs:\n",
    "    print (d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tokenize</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize text\n",
      "['We', 'are', 'running', 'a', 'tutorial', 'on', 'nlp', 'in', 'Python']\n",
      "['san_francisco', 'will', 'be', 'foggy', 'all', 'week']\n",
      "['Python', 'is', 'a', 'high', 'level', 'programming', 'language']\n",
      "['Today', 'it', 'is', 'cold', 'in', 'san_francisco']\n",
      "['It', 'is', 'so', 'sunny', 'that', 'you', 'can', 'fry', 'an', 'egg', 'outside', 'in', 'las_vegas']\n",
      "['las_vegas', 'is', 'getting', 'very', 'cold', 'in', 'the', 'evening']\n",
      "['Other', 'programming', 'languages', 'that', 'have', 'nlp', 'libraries', 'include', 'Java', 'and', 'C']\n",
      "['The', 'cold', 'weather', 'yesterday', 'evening', 'gave', 'Kevin', 'frostbite']\n",
      "['What', 'is', 'the', 'weather', 'outside', 'Is', 'it', 'rainy', 'or', 'sunny']\n",
      "[\"Doesn't\", 'it', 'cost', '$25', 'to', 'run', 'your', 'code', 'on', 'Amazon', 'No']\n",
      "['Running', 'nlp', 'code', 'is', 'sometimes', 'very', 'slow', 'and', 'not', 'very', 'accurate']\n",
      "['Python', 'libraries', 'for', 'nlp', 'include', 'Spacy', 'NLTK', 'and', 'gensim']\n",
      "['Gensim', 'is', 'is', 'used', 'in', 'this', 'tutorial', 'because', 'it', 'is', 'easy', 'to', 'install']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenize text\")\n",
    "\n",
    "tokens=[[word for word in doc.split() if word not in punct] for doc in docs]\n",
    "for d in tokens:\n",
    "    print (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lemmatization</h1>\n",
    "<p>Just to show that this step should be here</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Set\n",
      "['we', 'are', 'running', 'a', 'tutorial', 'on', 'nlp', 'in', 'python']\n",
      "['san_francisco', 'will', 'be', 'foggy', 'all', 'week']\n",
      "['python', 'is', 'a', 'high', 'level', 'programming', 'language']\n",
      "['today', 'it', 'is', 'cold', 'in', 'san_francisco']\n",
      "['it', 'is', 'so', 'sunny', 'that', 'you', 'can', 'fry', 'an', 'egg', 'outside', 'in', 'las_vegas']\n",
      "['las_vegas', 'is', 'getting', 'very', 'cold', 'in', 'the', 'evening']\n",
      "['other', 'programming', 'language', 'that', 'have', 'nlp', 'library', 'include', 'java', 'and', 'c']\n",
      "['the', 'cold', 'weather', 'yesterday', 'evening', 'gave', 'kevin', 'frostbite']\n",
      "['what', 'is', 'the', 'weather', 'outside', 'is', 'it', 'rainy', 'or', 'sunny']\n",
      "[\"doesn't\", 'it', 'cost', '$25', 'to', 'run', 'your', 'code', 'on', 'amazon', 'no']\n",
      "['running', 'nlp', 'code', 'is', 'sometimes', 'very', 'slow', 'and', 'not', 'very', 'accurate']\n",
      "['python', 'library', 'for', 'nlp', 'include', 'spacy', 'nltk', 'and', 'gensim']\n",
      "['gensim', 'is', 'is', 'used', 'in', 'this', 'tutorial', 'because', 'it', 'is', 'easy', 'to', 'install']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lemmaset = [[lmtzr.lemmatize(w.lower()) for w in sent] for sent in tokens]\n",
    "\n",
    "\n",
    "print(\"Lemmatized Set\")\n",
    "for s in lemmaset:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Remove Stopwords & Rare words</h1>\n",
    "<p>Add python code to separate out the set of stop words</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['We', 'are', 'running', 'tutorial', 'on', 'nlp', 'Python'], ['san_francisco', 'will', 'be', 'foggy', 'all', 'week'], ['Python', 'high', 'level', 'programming', 'language'], ['Today', 'cold', 'san_francisco'], ['so', 'sunny', 'that', 'you', 'can', 'fry', 'an', 'egg', 'outside', 'las_vegas'], ['las_vegas', 'getting', 'very', 'cold', 'evening'], ['Other', 'programming', 'languages', 'that', 'have', 'nlp', 'libraries', 'include', 'Java', 'C'], ['cold', 'weather', 'yesterday', 'evening', 'gave', 'Kevin', 'frostbite'], ['What', 'weather', 'outside', 'rainy', 'sunny'], [\"Doesn't\", 'cost', '$25', 'run', 'your', 'code', 'on', 'Amazon', 'No'], ['Running', 'nlp', 'code', 'sometimes', 'very', 'slow', 'not', 'very', 'accurate'], ['Python', 'libraries', 'nlp', 'include', 'Spacy', 'NLTK', 'gensim'], ['Gensim', 'used', 'this', 'tutorial', 'because', 'easy', 'install']]\n"
     ]
    }
   ],
   "source": [
    "stopset = set('for is a of the and to in it did or has had')\n",
    "filtered_docs = [[w for w in token if w.lower() not in stopset] for token in tokens]\n",
    "print(filtered_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower case\n",
      "['we', 'are', 'running', 'tutorial', 'on', 'nlp', 'python']\n",
      "['san_francisco', 'will', 'be', 'foggy', 'all', 'week']\n",
      "['python', 'high', 'level', 'programming', 'language']\n",
      "['today', 'cold', 'san_francisco']\n",
      "['so', 'sunny', 'that', 'you', 'can', 'fry', 'an', 'egg', 'outside', 'las_vegas']\n",
      "['las_vegas', 'getting', 'very', 'cold', 'evening']\n",
      "['other', 'programming', 'languages', 'that', 'have', 'nlp', 'libraries', 'include', 'java', 'c']\n",
      "['cold', 'weather', 'yesterday', 'evening', 'gave', 'kevin', 'frostbite']\n",
      "['what', 'weather', 'outside', 'rainy', 'sunny']\n",
      "[\"doesn't\", 'cost', '$25', 'run', 'your', 'code', 'on', 'amazon', 'no']\n",
      "['running', 'nlp', 'code', 'sometimes', 'very', 'slow', 'not', 'very', 'accurate']\n",
      "['python', 'libraries', 'nlp', 'include', 'spacy', 'nltk', 'gensim']\n",
      "['gensim', 'used', 'this', 'tutorial', 'because', 'easy', 'install']\n",
      "\n",
      "Unique words\n",
      "{'an', 'rainy', 'kevin', 'language', 'we', 'this', 'so', 'java', 'high', 'fry', 'accurate', 'sometimes', 'yesterday', 'have', 'amazon', 'foggy', 'languages', 'will', 'level', 'other', 'you', 'cost', 'install', 'c', 'gave', 'easy', 'egg', 'run', 'not', 'your', 'nltk', 'slow', 'spacy', 'today', 'can', 'be', \"doesn't\", 'getting', 'week', 'used', 'all', 'what', 'because', 'no', 'frostbite', '$25', 'are'}\n"
     ]
    }
   ],
   "source": [
    "#case insensitive\n",
    "idocs =[[word.lower() for word in doc] for doc in filtered_docs]\n",
    "print(\"Lower case\")\n",
    "for i in idocs:\n",
    "    print(i)\n",
    "print('')\n",
    "\n",
    "\n",
    "print('Unique words')\n",
    "alltokens =sum(idocs,[])\n",
    "uniqueset = set(word for word in set(alltokens) if alltokens.count(word) == 1)\n",
    "\n",
    "print(uniqueset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words\n",
      "['running', 'tutorial', 'on', 'nlp', 'python']\n",
      "['san_francisco']\n",
      "['python', 'programming']\n",
      "['cold', 'san_francisco']\n",
      "['sunny', 'that', 'outside', 'las_vegas']\n",
      "['las_vegas', 'very', 'cold', 'evening']\n",
      "['programming', 'that', 'nlp', 'libraries', 'include']\n",
      "['cold', 'weather', 'evening']\n",
      "['weather', 'outside', 'sunny']\n",
      "['code', 'on']\n",
      "['running', 'nlp', 'code', 'very', 'very']\n",
      "['python', 'libraries', 'nlp', 'include', 'gensim']\n",
      "['gensim', 'tutorial']\n"
     ]
    }
   ],
   "source": [
    "wbag = [[w for w in text if w not in uniqueset] for text in idocs]\n",
    "\n",
    "print('Bag of words')\n",
    "for i in wbag:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Dictionary</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(19 unique tokens: ['python', 'tutorial', 'san_francisco', 'programming', 'on']...)\n",
      "\n",
      "{'python': 3, 'tutorial': 1, 'san_francisco': 5, 'programming': 6, 'on': 4, 'gensim': 18, 'very': 12, 'sunny': 9, 'that': 10, 'outside': 11, 'nlp': 0, 'cold': 7, 'include': 15, 'running': 2, 'las_vegas': 8, 'libraries': 14, 'weather': 16, 'code': 17, 'evening': 13}\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(wbag)\n",
    "print(dictionary)\n",
    "print('')\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n",
      "[(5, 1)]\n",
      "[(3, 1), (6, 1)]\n",
      "[(5, 1), (7, 1)]\n",
      "[(8, 1), (9, 1), (10, 1), (11, 1)]\n",
      "[(7, 1), (8, 1), (12, 1), (13, 1)]\n",
      "[(0, 1), (6, 1), (10, 1), (14, 1), (15, 1)]\n",
      "[(7, 1), (13, 1), (16, 1)]\n",
      "[(9, 1), (11, 1), (16, 1)]\n",
      "[(4, 1), (17, 1)]\n",
      "[(0, 1), (2, 1), (12, 2), (17, 1)]\n",
      "[(0, 1), (3, 1), (14, 1), (15, 1), (18, 1)]\n",
      "[(1, 1), (18, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in wbag] \n",
    "for i in corpus:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tf-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model\n",
      "TfidfModel(num_docs=13, num_nnz=42)\n",
      "\n",
      "[(0, 0.3144444033650505), (1, 0.4993638686130151), (2, 0.4993638686130151), (3, 0.3911929157895468), (4, 0.4993638686130151)]\n",
      "[(5, 1.0)]\n",
      "[(3, 0.6166859611993709), (6, 0.7872092639569278)]\n",
      "[(5, 0.7872092639569278), (7, 0.6166859611993709)]\n",
      "[(8, 0.5), (9, 0.5), (10, 0.5), (11, 0.5)]\n",
      "[(7, 0.41209612453809086), (8, 0.5260471411514012), (12, 0.5260471411514012), (13, 0.5260471411514012)]\n",
      "[(0, 0.30031204378750365), (6, 0.47692050604796093), (10, 0.47692050604796093), (14, 0.47692050604796093), (15, 0.47692050604796093)]\n",
      "[(7, 0.4845593542465289), (13, 0.6185475859673962), (16, 0.6185475859673962)]\n",
      "[(9, 0.5773502691896257), (11, 0.5773502691896257), (16, 0.5773502691896257)]\n",
      "[(4, 0.7071067811865476), (17, 0.7071067811865476)]\n",
      "[(0, 0.2489747079866609), (2, 0.3953925464613104), (12, 0.7907850929226208), (17, 0.3953925464613104)]\n",
      "[(0, 0.3144444033650505), (3, 0.3911929157895468), (14, 0.4993638686130151), (15, 0.4993638686130151), (18, 0.4993638686130151)]\n",
      "[(1, 0.7071067811865476), (18, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "#initialize\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "print(\"TF-IDF Model\")\n",
    "print(tfidf)\n",
    "print(\"\")\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "for i in corpus_tfidf:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Latent Dirichlet Allocation</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 0.076068650367158319), (12, 0.068446842112621836), (13, 0.06829356070027498), (5, 0.067104028133154442), (16, 0.064699498143273568), (8, 0.063479102882314981), (9, 0.057023729231179342), (6, 0.056192114730554764), (11, 0.055460448856976319), (10, 0.052930387783197287)]\n",
      "[(4, 0.075025227416815624), (3, 0.068376646878450076), (5, 0.067136706989446598), (17, 0.066451139344808466), (0, 0.063605875212852533), (1, 0.062075469862605363), (15, 0.057804974263146479), (2, 0.057340675626576451), (14, 0.05683545887949245), (18, 0.056203782469032723)]\n"
     ]
    }
   ],
   "source": [
    "tops = 2\n",
    "lda = models.LdaModel(corpus_tfidf, id2word = dictionary, num_topics = tops)\n",
    "\n",
    "topic = [lda.get_topic_terms(t) for t in range(tops)]\n",
    "for i in topic:\n",
    "    print (i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.076*cold + 0.068*very + 0.068*evening + 0.067*san_francisco + 0.065*weather + 0.063*las_vegas + 0.057*sunny + 0.056*programming + 0.055*outside + 0.053*that'),\n",
       " (1,\n",
       "  '0.075*on + 0.068*python + 0.067*san_francisco + 0.066*code + 0.064*nlp + 0.062*tutorial + 0.058*include + 0.057*running + 0.057*libraries + 0.056*gensim')]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.1918802947330393), (1, 0.80811970526696064)]\n",
      "[(0, 0.53266630927686653), (1, 0.46733369072313347)]\n",
      "[(0, 0.39990993673337794), (1, 0.60009006326662206)]\n",
      "[(0, 0.71027144969227451), (1, 0.28972855030772543)]\n",
      "[(0, 0.79387628850465364), (1, 0.20612371149534628)]\n",
      "[(0, 0.81205872506854326), (1, 0.18794127493145671)]\n",
      "[(0, 0.29947136752391357), (1, 0.70052863247608643)]\n",
      "[(0, 0.79205025270582086), (1, 0.20794974729417917)]\n",
      "[(0, 0.77183055508636944), (1, 0.22816944491363056)]\n",
      "[(0, 0.23365680759143373), (1, 0.76634319240856641)]\n",
      "[(0, 0.45618784270500912), (1, 0.54381215729499088)]\n",
      "[(0, 0.21257252054613801), (1, 0.78742747945386204)]\n",
      "[(0, 0.31537543571999294), (1, 0.68462456428000706)]\n"
     ]
    }
   ],
   "source": [
    "lda_corpus = lda[corpus_tfidf]\n",
    "for doc in lda_corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
