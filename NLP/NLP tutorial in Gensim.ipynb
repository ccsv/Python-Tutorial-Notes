{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1 align=\"center\">NLP Tutorial in Gensim</h1>\n",
    "<h3 align=\"center\">Chuck Chan</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are running a tutorial on Natural Language Processing in Python.\n",
      "San Francisco will be foggy all week!\n",
      "Python is a high level programming language.\n",
      "Today it is cold in SF\n",
      "It is so sunny that you can fry an egg outside in Vegas.\n",
      "Las Vegas is getting very cold in the evening.\n",
      "Other programming languages that have Natural Language Processing libraries include Java and C.\n",
      "The cold weather yesterday evening gave Kevin frostbite.\n",
      "What is the weather outside? Is it rainy or sunny?\n",
      "Doesn't it cost $25 to run your code on Amazon? No\n",
      "Running NLP code is sometimes very slow and not very accurate.\n",
      "Python libraries for NLP include: Spacy, NLTK, and gensim\n",
      "Gensim is is used in this tutorial because it is easy to install\n"
     ]
    }
   ],
   "source": [
    "#We are using these sentences as examples to represent documents to save time. \n",
    "#But the results for the topic model will be biased because this is intended for large documents\n",
    "#where words are distributed differently\n",
    "docs =[\"We are running a tutorial on Natural Language Processing in Python.\",\n",
    "       \"San Francisco will be foggy all week!\", \n",
    "       \"Python is a high level programming language.\",\n",
    "       \"Today it is cold in SF\",\n",
    "       \"It is so sunny that you can fry an egg outside in Vegas.\",\n",
    "       \"Las Vegas is getting very cold in the evening.\",\n",
    "       \"Other programming languages that have Natural Language Processing libraries include Java and C.\",\n",
    "       \"The cold weather yesterday evening gave Kevin frostbite.\",\n",
    "       \"What is the weather outside? Is it rainy or sunny?\",\n",
    "       \"Doesn't it cost $25 to run your code on Amazon? No\",\n",
    "       \"Running NLP code is sometimes very slow and not very accurate.\",\n",
    "       \"Python libraries for NLP include: Spacy, NLTK, and gensim\",\n",
    "       \"Gensim is is used in this tutorial because it is easy to install\",\n",
    "    ]\n",
    "for n in docs:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Remove punctuation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are running a tutorial on Natural Language Processing in Python\n",
      "San Francisco will be foggy all week\n",
      "Python is a high level programming language\n",
      "Today it is cold in SF\n",
      "It is so sunny that you can fry an egg outside in Vegas\n",
      "Las Vegas is getting very cold in the evening\n",
      "Other programming languages that have Natural Language Processing libraries include Java and C\n",
      "The cold weather yesterday evening gave Kevin frostbite\n",
      "What is the weather outside Is it rainy or sunny\n",
      "Doesn't it cost $25 to run your code on Amazon No\n",
      "Running NLP code is sometimes very slow and not very accurate\n",
      "Python libraries for NLP include Spacy NLTK and gensim\n",
      "Gensim is is used in this tutorial because it is easy to install\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replace punctuation\n",
    "import re\n",
    "for i in range(len(docs)):\n",
    "    docs[i]= re.sub('(\\w+)(?:\\?|\\,|\\.|:|;|!)','\\g<1>', docs[i])\n",
    "    print(docs[i])\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dealing with compound Words</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are running a tutorial on nlp in Python\n",
      "san_francisco will be foggy all week\n",
      "Python is a high level programming language\n",
      "Today it is cold in san_francisco\n",
      "It is so sunny that you can fry an egg outside in las_vegas\n",
      "las_vegas is getting very cold in the evening\n",
      "Other programming languages that have nlp libraries include Java and C\n",
      "The cold weather yesterday evening gave Kevin frostbite\n",
      "What is the weather outside Is it rainy or sunny\n",
      "Doesn't it cost $25 to run your code on Amazon No\n",
      "Running nlp code is sometimes very slow and not very accurate\n",
      "Python libraries for nlp include Spacy NLTK and gensim\n",
      "Gensim is is used in this tutorial because it is easy to install\n"
     ]
    }
   ],
   "source": [
    "replacements ={\n",
    "    'san_francisco':['San Francisco', 'SF'],\n",
    "    'las_vegas':['Las Vegas',\"Vegas\"],\n",
    "    'nlp':['NLP', 'Natural Language Processing']\n",
    "}\n",
    "\n",
    "    \n",
    "    \n",
    "for key, value in replacements.items():\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(value)):\n",
    "            docs[i] = docs[i].replace(value[j], key)\n",
    "            \n",
    "for d in docs:\n",
    "    print (d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tokenize</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize text\n",
      "['We', 'are', 'running', 'a', 'tutorial', 'on', 'nlp', 'in', 'Python']\n",
      "['san_francisco', 'will', 'be', 'foggy', 'all', 'week']\n",
      "['Python', 'is', 'a', 'high', 'level', 'programming', 'language']\n",
      "['Today', 'it', 'is', 'cold', 'in', 'san_francisco']\n",
      "['It', 'is', 'so', 'sunny', 'that', 'you', 'can', 'fry', 'an', 'egg', 'outside', 'in', 'las_vegas']\n",
      "['las_vegas', 'is', 'getting', 'very', 'cold', 'in', 'the', 'evening']\n",
      "['Other', 'programming', 'languages', 'that', 'have', 'nlp', 'libraries', 'include', 'Java', 'and', 'C']\n",
      "['The', 'cold', 'weather', 'yesterday', 'evening', 'gave', 'Kevin', 'frostbite']\n",
      "['What', 'is', 'the', 'weather', 'outside', 'Is', 'it', 'rainy', 'or', 'sunny']\n",
      "[\"Doesn't\", 'it', 'cost', '$25', 'to', 'run', 'your', 'code', 'on', 'Amazon', 'No']\n",
      "['Running', 'nlp', 'code', 'is', 'sometimes', 'very', 'slow', 'and', 'not', 'very', 'accurate']\n",
      "['Python', 'libraries', 'for', 'nlp', 'include', 'Spacy', 'NLTK', 'and', 'gensim']\n",
      "['Gensim', 'is', 'is', 'used', 'in', 'this', 'tutorial', 'because', 'it', 'is', 'easy', 'to', 'install']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenize text\")\n",
    "\n",
    "tokens=[[word for word in doc.split()] for doc in docs]\n",
    "for d in tokens:\n",
    "    print (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lemmatization</h1>\n",
    "For demonstration purposes to show when it is applied and function. Change libraries to library and languages to language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Set\n",
      "['we', 'are', 'running', 'a', 'tutorial', 'on', 'nlp', 'in', 'python']\n",
      "['san_francisco', 'will', 'be', 'foggy', 'all', 'week']\n",
      "['python', 'is', 'a', 'high', 'level', 'programming', 'language']\n",
      "['today', 'it', 'is', 'cold', 'in', 'san_francisco']\n",
      "['it', 'is', 'so', 'sunny', 'that', 'you', 'can', 'fry', 'an', 'egg', 'outside', 'in', 'las_vegas']\n",
      "['las_vegas', 'is', 'getting', 'very', 'cold', 'in', 'the', 'evening']\n",
      "['other', 'programming', 'language', 'that', 'have', 'nlp', 'library', 'include', 'java', 'and', 'c']\n",
      "['the', 'cold', 'weather', 'yesterday', 'evening', 'gave', 'kevin', 'frostbite']\n",
      "['what', 'is', 'the', 'weather', 'outside', 'is', 'it', 'rainy', 'or', 'sunny']\n",
      "[\"doesn't\", 'it', 'cost', '$25', 'to', 'run', 'your', 'code', 'on', 'amazon', 'no']\n",
      "['running', 'nlp', 'code', 'is', 'sometimes', 'very', 'slow', 'and', 'not', 'very', 'accurate']\n",
      "['python', 'library', 'for', 'nlp', 'include', 'spacy', 'nltk', 'and', 'gensim']\n",
      "['gensim', 'is', 'is', 'used', 'in', 'this', 'tutorial', 'because', 'it', 'is', 'easy', 'to', 'install']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lemmaset = [[lmtzr.lemmatize(w.lower()) for w in sent] for sent in tokens]\n",
    "\n",
    "\n",
    "print(\"Lemmatized Set\")\n",
    "for s in lemmaset:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Remove Stopwords & Rare words</h1>\n",
    "Rare words rmoved to prevent biasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['We', 'are', 'running', 'tutorial', 'on', 'nlp', 'Python'], ['san_francisco', 'will', 'be', 'foggy', 'all', 'week'], ['Python', 'high', 'level', 'programming', 'language'], ['Today', 'cold', 'san_francisco'], ['so', 'sunny', 'that', 'you', 'can', 'fry', 'an', 'egg', 'outside', 'las_vegas'], ['las_vegas', 'getting', 'very', 'cold', 'evening'], ['Other', 'programming', 'languages', 'that', 'have', 'nlp', 'libraries', 'include', 'Java', 'C'], ['cold', 'weather', 'yesterday', 'evening', 'gave', 'Kevin', 'frostbite'], ['What', 'weather', 'outside', 'rainy', 'sunny'], [\"Doesn't\", 'cost', '$25', 'run', 'your', 'code', 'on', 'Amazon', 'No'], ['Running', 'nlp', 'code', 'sometimes', 'very', 'slow', 'not', 'very', 'accurate'], ['Python', 'libraries', 'nlp', 'include', 'Spacy', 'NLTK', 'gensim'], ['Gensim', 'used', 'this', 'tutorial', 'because', 'easy', 'install']]\n"
     ]
    }
   ],
   "source": [
    "stopset = set('for is a of the and to in it did or has had'.split())\n",
    "filtered_docs = [[w for w in token if w.lower() not in stopset] for token in tokens]\n",
    "print(filtered_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower case\n",
      "['we', 'are', 'running', 'tutorial', 'on', 'nlp', 'python']\n",
      "['san_francisco', 'will', 'be', 'foggy', 'all', 'week']\n",
      "['python', 'high', 'level', 'programming', 'language']\n",
      "['today', 'cold', 'san_francisco']\n",
      "['so', 'sunny', 'that', 'you', 'can', 'fry', 'an', 'egg', 'outside', 'las_vegas']\n",
      "['las_vegas', 'getting', 'very', 'cold', 'evening']\n",
      "['other', 'programming', 'languages', 'that', 'have', 'nlp', 'libraries', 'include', 'java', 'c']\n",
      "['cold', 'weather', 'yesterday', 'evening', 'gave', 'kevin', 'frostbite']\n",
      "['what', 'weather', 'outside', 'rainy', 'sunny']\n",
      "[\"doesn't\", 'cost', '$25', 'run', 'your', 'code', 'on', 'amazon', 'no']\n",
      "['running', 'nlp', 'code', 'sometimes', 'very', 'slow', 'not', 'very', 'accurate']\n",
      "['python', 'libraries', 'nlp', 'include', 'spacy', 'nltk', 'gensim']\n",
      "['gensim', 'used', 'this', 'tutorial', 'because', 'easy', 'install']\n",
      "\n",
      "Unique words\n",
      "{'week', 'run', 'high', 'languages', 'not', 'rainy', 'no', 'getting', 'egg', 'language', 'yesterday', 'level', \"doesn't\", 'gave', 'other', 'cost', 'easy', 'fry', 'are', 'you', 'nltk', 'can', 'slow', 'your', 'will', 'kevin', 'c', 'spacy', 'we', 'have', 'so', 'frostbite', 'foggy', 'java', 'all', 'because', 'an', '$25', 'what', 'used', 'be', 'today', 'this', 'install', 'accurate', 'sometimes', 'amazon'}\n"
     ]
    }
   ],
   "source": [
    "#case insensitive\n",
    "idocs =[[word.lower() for word in doc] for doc in filtered_docs]\n",
    "print(\"Lower case\")\n",
    "for i in idocs:\n",
    "    print(i)\n",
    "print('')\n",
    "\n",
    "\n",
    "print('Unique words')\n",
    "alltokens =sum(idocs,[])\n",
    "uniqueset = set(word for word in set(alltokens) if alltokens.count(word) == 1)\n",
    "\n",
    "print(uniqueset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words\n",
      "['running', 'tutorial', 'on', 'nlp', 'python']\n",
      "['san_francisco']\n",
      "['python', 'programming']\n",
      "['cold', 'san_francisco']\n",
      "['sunny', 'that', 'outside', 'las_vegas']\n",
      "['las_vegas', 'very', 'cold', 'evening']\n",
      "['programming', 'that', 'nlp', 'libraries', 'include']\n",
      "['cold', 'weather', 'evening']\n",
      "['weather', 'outside', 'sunny']\n",
      "['code', 'on']\n",
      "['running', 'nlp', 'code', 'very', 'very']\n",
      "['python', 'libraries', 'nlp', 'include', 'gensim']\n",
      "['gensim', 'tutorial']\n"
     ]
    }
   ],
   "source": [
    "wbag = [[w for w in text if w not in uniqueset] for text in idocs]\n",
    "\n",
    "print('Bag of words')\n",
    "for i in wbag:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text Dictionary</h1>\n",
    "Done to save memory by since reading strngs are more expensive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(19 unique tokens: ['libraries', 'gensim', 'nlp', 'include', 'sunny']...)\n",
      "\n",
      "{'libraries': 14, 'gensim': 18, 'nlp': 2, 'include': 15, 'sunny': 9, 'cold': 7, 'very': 12, 'that': 10, 'evening': 13, 'on': 0, 'python': 1, 'programming': 6, 'outside': 8, 'tutorial': 4, 'las_vegas': 11, 'san_francisco': 5, 'code': 17, 'running': 3, 'weather': 16}\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(wbag)\n",
    "print(dictionary)\n",
    "print('')\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]\n",
      "[(5, 1)]\n",
      "[(1, 1), (6, 1)]\n",
      "[(5, 1), (7, 1)]\n",
      "[(8, 1), (9, 1), (10, 1), (11, 1)]\n",
      "[(7, 1), (11, 1), (12, 1), (13, 1)]\n",
      "[(2, 1), (6, 1), (10, 1), (14, 1), (15, 1)]\n",
      "[(7, 1), (13, 1), (16, 1)]\n",
      "[(8, 1), (9, 1), (16, 1)]\n",
      "[(0, 1), (17, 1)]\n",
      "[(2, 1), (3, 1), (12, 2), (17, 1)]\n",
      "[(1, 1), (2, 1), (14, 1), (15, 1), (18, 1)]\n",
      "[(4, 1), (18, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in wbag]\n",
    "\n",
    "#Display for each doc[(word, frequency)]\n",
    "for i in corpus:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tf-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Model\n",
      "TfidfModel(num_docs=13, num_nnz=42)\n",
      "\n",
      "[(0, 0.4993638686130151), (1, 0.3911929157895468), (2, 0.3144444033650505), (3, 0.4993638686130151), (4, 0.4993638686130151)]\n",
      "[(5, 1.0)]\n",
      "[(1, 0.6166859611993709), (6, 0.7872092639569278)]\n",
      "[(5, 0.7872092639569278), (7, 0.6166859611993709)]\n",
      "[(8, 0.5), (9, 0.5), (10, 0.5), (11, 0.5)]\n",
      "[(7, 0.41209612453809086), (11, 0.5260471411514012), (12, 0.5260471411514012), (13, 0.5260471411514012)]\n",
      "[(2, 0.30031204378750365), (6, 0.47692050604796093), (10, 0.47692050604796093), (14, 0.47692050604796093), (15, 0.47692050604796093)]\n",
      "[(7, 0.4845593542465289), (13, 0.6185475859673962), (16, 0.6185475859673962)]\n",
      "[(8, 0.5773502691896257), (9, 0.5773502691896257), (16, 0.5773502691896257)]\n",
      "[(0, 0.7071067811865476), (17, 0.7071067811865476)]\n",
      "[(2, 0.2489747079866609), (3, 0.3953925464613104), (12, 0.7907850929226208), (17, 0.3953925464613104)]\n",
      "[(1, 0.3911929157895468), (2, 0.3144444033650505), (14, 0.4993638686130151), (15, 0.4993638686130151), (18, 0.4993638686130151)]\n",
      "[(4, 0.7071067811865476), (18, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "#initialize\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "print(\"TF-IDF Model\")\n",
    "print(tfidf)\n",
    "print(\"\")\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "for i in corpus_tfidf:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Latent Dirichlet Allocation</h1>\n",
    "LDA is an unsupervised method for topic modeling where output are probabilities for each topic.\n",
    "\n",
    "See <a href=\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">LDA</a> for description\n",
    "For additional gensim lda model features see https://radimrehurek.com/gensim/models/ldamodel.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.076376273133250308), (17, 0.064164492677973375), (1, 0.064043505272563098), (6, 0.062417511825050481), (5, 0.059349664650496618), (2, 0.055736455753149505), (10, 0.055656404999788488), (7, 0.053746928423660613), (4, 0.05288320770551494), (3, 0.052513164626118207)]\n",
      "[(5, 0.073762955257936247), (7, 0.06631938025319381), (18, 0.06485885199195654), (16, 0.063131086135859715), (13, 0.062654159547379254), (12, 0.061373080465044236), (8, 0.055146858175267328), (9, 0.054382466098103467), (4, 0.05334932843150568), (1, 0.052410471876820255)]\n"
     ]
    }
   ],
   "source": [
    "#Define the number of topics\n",
    "tops = 2\n",
    "#setup model\n",
    "lda = models.LdaModel(corpus_tfidf, id2word = dictionary, num_topics = tops)\n",
    "\n",
    "#shows component of each topic\n",
    "topic = [lda.get_topic_terms(t) for t in range(tops)]\n",
    "for i in topic:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.076*on + 0.064*code + 0.064*python + 0.062*programming + 0.059*san_francisco + 0.056*nlp + 0.056*that + 0.054*cold + 0.053*tutorial + 0.053*running'),\n",
       " (1,\n",
       "  '0.074*san_francisco + 0.066*cold + 0.065*gensim + 0.063*weather + 0.063*evening + 0.061*very + 0.055*outside + 0.054*sunny + 0.053*tutorial + 0.052*python')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows component of each topic\n",
    "lda.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.77027836683922535), (1, 0.22972163316077465)]\n",
      "[(0, 0.34116897476046526), (1, 0.65883102523953474)]\n",
      "[(0, 0.69089022686458967), (1, 0.30910977313541027)]\n",
      "[(0, 0.28745066666565844), (1, 0.71254933333434167)]\n",
      "[(0, 0.31224269904431767), (1, 0.68775730095568222)]\n",
      "[(0, 0.22412969751308767), (1, 0.77587030248691236)]\n",
      "[(0, 0.72864670710449231), (1, 0.27135329289550769)]\n",
      "[(0, 0.22371888459291056), (1, 0.77628111540708955)]\n",
      "[(0, 0.2349834983299168), (1, 0.76501650167008306)]\n",
      "[(0, 0.76254417071355896), (1, 0.23745582928644118)]\n",
      "[(0, 0.57075841096368718), (1, 0.42924158903631271)]\n",
      "[(0, 0.31646820560394512), (1, 0.68353179439605494)]\n",
      "[(0, 0.2776026713704336), (1, 0.7223973286295664)]\n"
     ]
    }
   ],
   "source": [
    "#For each doc displays [(topic1, probability), (topic2, probability)]\n",
    "lda_corpus = lda[corpus_tfidf]\n",
    "for doc in lda_corpus:\n",
    "    print(doc) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
